{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e01a0e-bafc-4ba4-aa96-ba5624db272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "\n",
    "class HandDetector:\n",
    "    def __init__(self, static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5):\n",
    "        # Initialise HandDetector object with parameters for hand tracking\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.hands = self.mp_hands.Hands(static_image_mode=static_image_mode, max_num_hands=max_num_hands,\n",
    "                                          min_detection_confidence=min_detection_confidence,\n",
    "                                          min_tracking_confidence=min_tracking_confidence)\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    def detect_landmarks(self, frame):\n",
    "        # Convert BGR frame to RGB for compatibility with MediaPipe\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # Process the frame to detect hand landmarks\n",
    "        results = self.hands.process(image_rgb)\n",
    "        landmarks = []\n",
    "        if results.multi_hand_landmarks:\n",
    "            # Select the hand with the highest confidence score (determining which detected hand in the frame is most likely to be accurate)\n",
    "            max_confidence_hand = max(results.multi_handedness, key=lambda x: x.classification[0].score)\n",
    "            max_confidence_index = results.multi_handedness.index(max_confidence_hand)\n",
    "            hand_landmarks = results.multi_hand_landmarks[max_confidence_index]\n",
    "            landmarks.append(hand_landmarks.landmark)\n",
    "            # Draw hand landmarks on the frame\n",
    "            self.mp_drawing.draw_landmarks(frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)\n",
    "        return frame, landmarks\n",
    "\n",
    "class CursorController:\n",
    "    def __init__(self, screen_width, screen_height):\n",
    "        # Initialise CursorController object with screen dimensions\n",
    "        self.screen_width = screen_width\n",
    "        self.screen_height = screen_height\n",
    "        self.is_fist = False\n",
    "\n",
    "    def move_cursor(self, x, y):\n",
    "        # Move the cursor to the target position based on hand gestures\n",
    "        target_x = int((1 - x) * self.screen_width)  # Invert x-axis as this was going the wrong way\n",
    "        target_y = int(y * self.screen_height)\n",
    "        pyautogui.moveTo(target_x, target_y)\n",
    "\n",
    "    def click(self):\n",
    "        # Simulate a mouse click\n",
    "        pyautogui.mouseDown()\n",
    "\n",
    "    def release_click(self):\n",
    "        # Release the mouse click\n",
    "        pyautogui.mouseUp()\n",
    "\n",
    "def main():\n",
    "    # Initialise video capture object\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    # Get screen resolution\n",
    "    screen_width = pyautogui.size().width\n",
    "    screen_height = pyautogui.size().height\n",
    "    # Initialise HandDetector and CursorController objects\n",
    "    hand_detector = HandDetector()\n",
    "    cursor_controller = CursorController(screen_width, screen_height)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        # Read frame from video capture\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Detect hand landmarks in the frame\n",
    "        frame, landmarks = hand_detector.detect_landmarks(frame)\n",
    "\n",
    "        if landmarks:\n",
    "            thumb_tip = (landmarks[0][4].x, landmarks[0][4].y)\n",
    "            index_tip = (landmarks[0][8].x, landmarks[0][8].y)\n",
    "            distance = ((thumb_tip[0] - index_tip[0])**2 + (thumb_tip[1] - index_tip[1])**2) ** 0.5\n",
    "            if distance < 0.05:  # Adjusted threshold for pinching fingers to left click\n",
    "                if not cursor_controller.is_fist:\n",
    "                    cursor_controller.click()\n",
    "                    cursor_controller.is_fist = True\n",
    "            else:\n",
    "                if cursor_controller.is_fist:\n",
    "                    cursor_controller.release_click()\n",
    "                    cursor_controller.is_fist = False\n",
    "\n",
    "            palm_center = (landmarks[0][0].x, landmarks[0][0].y)  # Assuming the first landmark is the palm center.\n",
    "            cursor_controller.move_cursor(palm_center[0], palm_center[1])\n",
    "\n",
    "        # Display frame with hand landmarks\n",
    "        cv2.imshow('Hand Tracking', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release video capture object and close all OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d383f9-e57d-410b-b2b1-32a096ce7bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
